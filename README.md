# **SecureFed**
A federated learning framework built with Flower and PyTorch to evaluate the robustness of FL systems under data poisoning attacks. The system uses the PathMNIST dataset from the MedMNIST benchmark, simulates realistic adversarial scenarios by flipping labels on malicious clients, and defends against poisoned updates using a trust and reputation-based aggregation strategy.

## **ğŸš€Highlights**
- ğŸ§  **CNN-based Federated Learning**: Trains CNN models (e.g., ResNet18) in a non-IID federated setting.
- ğŸ”„ **Adversarial Simulation**: Simulates data poisoning attacks by flipping labels on compromised clients.
- ğŸ›¡ï¸ **Defense via Trust & Reputation**: Excludes unreliable clients using trust and reputation scoring.
- ğŸ“‰ **Metric Logging**: Logs accuracy, loss, and trust scores; saves model checkpoints each round.
- âš¡ **GPU-Accelerated**: Optimized for CUDA-enabled training on GPUs.

## **â–¶ï¸Running the Project**

### 1. **Requirements**:
The `requirements.txt` file contains all the necessary Python dependencies for the project. Place any additional required dependencies here. You can install them using:
``` bash
    pip install -r requirements.txt
```

### 2. **Run the Full Federated Learning Setup**
You can start both the server and multiple clients using the provided script:
```bash
./run.sh <number_of_clients>
```

For Attack simulation 
``` bash
nohup python3 -u main_client.py $i 1 > logs/client_$i.log 2>&1 &
```
By default, run.sh starts clients with attack_flag=1. Replace 1 with 0 to disable label-flipping attacks for clients. You can also customize the sleep duration or logging paths if needed.

## **ğŸ—‚ï¸Project Structure**
```bash
    â”‚   main_client.py
    â”‚   main_server.py
    â”‚   plot.py
    â”‚   pyproject.toml
    â”‚   README.md
    â”‚   requirements.txt
    â”‚   run.sh
    â”‚   
    â”œâ”€â”€â”€backend
    â”‚      client_app.py
    â”‚      distance.py
    â”‚      model.py
    â”‚      save_model_strategy.py
    â”‚      task.py
    â”‚
    â”œâ”€â”€â”€checkpoints
    â”‚       model_round_1.pth
    â”‚       model_round_2.pth
    â”‚
    â””â”€â”€â”€logs
            client_0.log
            client_1.log
            server.log
 ```

 ## **ğŸ“ŠResults**

We evaluated the robustness of our federated learning setup across three scenarios:

---

### âœ… Scenario 1: No Attack (Standard FL)

- âš™ï¸ **Setup**: 10 clients, clean data, 10 rounds, 5 local epochs per round.
- ğŸ“ˆ **Accuracy**: Reached approximately **80%**.
- ğŸ“‰ **Loss**: Decreased steadily and remained **below 2.0** throughout training.
- ğŸ§  **Insight**: The global model demonstrated stable convergence and strong performance under clean conditions.

---

### âŒ Scenario 2: With Attack (No Defense)

- ğŸ§ª **Attack Simulation**: 3 out of 10 clients had **50% of their labels flipped** to simulate data poisoning.
- âš ï¸ **Impact**:
  - First-round **loss** for malicious clients was **~11.52**, compared to **< 5** for clean clients.
  - The global model's **loss** gradually decreased but stabilized at **~2.2** after 10 rounds.
  - **Accuracy** only reached **~55â€“60%**, far below the clean setup.
- ğŸ” **Insight**: Poisoned updates significantly slowed down convergence and reduced global performance.

---

### ğŸ›¡ï¸ Scenario 3: With Attack + Trust & Reputation Defense

- ğŸ” **Defense**:
  - Trust and reputation scores were computed in early rounds.
  - From **Round 4**, clients with low trust scores were excluded from aggregation.
- ğŸ“ˆ **Recovery**:
  - Global **accuracy** recovered to **~70%** by Round 10.
  - Model began to converge more reliably after filtering out malicious contributions.
- âœ… **Insight**: The defense mechanism improved accuracy and mitigated attack effects, though it didnâ€™t fully match the clean scenario.

---

### ğŸ“Œ Summary Table

| Scenario              | Accuracy     | Loss     | Remarks                                |
|-----------------------|--------------|----------|----------------------------------------|
| No Attack             | ~80%         | < 2.0    | Smooth convergence                      |
| Attack, No Defense    | ~55â€“60%      | ~2.2     | High instability, slow convergence      |
| Attack + Defense      | ~70%         | ~2.0     | Partial recovery, improved robustness   |

ğŸ“„ **[Read the full report](./report/securefed_report.pdf)** for methodology, experiments and results.